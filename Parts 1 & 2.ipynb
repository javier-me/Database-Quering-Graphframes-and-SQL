{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIikFr7cAFkq"
   },
   "source": [
    "# ST446 Distributed Computing for Big Data\n",
    "---\n",
    "\n",
    "*We highly recommend using GCP, as the data sets used are about 20 GB in total.* Alternatively, you can use your own computer.\n",
    "\n",
    "## Querying the YAGO semantic knowledge base\n",
    "\n",
    "YAGO is a semantic knowledge base, derived from Wikipedia, WordNet and GeoNames. YAGO contains knowledge about more than 10 million entities (like persons, organizations and cities) and contains more than 120 million facts about these entities. You may find more about YAGO [here](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/#c10444).\n",
    "\n",
    "In this assignment, you are asked to use parts of the YAGO dataset to demonstrate your knowledge about Spark graphframes and motif queries. In particular, you are asked to **_use motif queries_** to find out answers to the following queries stated in English:\n",
    "\n",
    "**A**. _Politicians who are also scientists_ (sorted alphabetically by name of person)\n",
    "\n",
    "**B**. _Companies whose founders were born in London_ (sorted alphabetically by name of founder)\n",
    "\n",
    "**C**. _Writers who have won a Nobel Prize (in any discipline)_ (sorted alphabetically by name of person)\n",
    "\n",
    "**D**. _Nobel prize winners who were born in the same city as their spouses_ (sorted alphabetically by name of person)\n",
    "\n",
    "**E**. _Politicians that are affiliated with a right-wing party_ (sorted alphabetically by name of person)\n",
    "\n",
    "Please always show the first 20 entries of the resulting DataFrame and the total count of relevant entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fz6Fy6jfAFkr"
   },
   "source": [
    "## 0.1 Get YAGO data\n",
    "\n",
    "You will need to download the following datasets that are part of YAGO (see [here](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/downloads/) for more information):\n",
    "\n",
    "* A set of relationships between instances (for example, specifying that Emomali Rahmon is the leader of the Military of Tajikistan). Link: http://resources.mpi-inf.mpg.de/yago-naga/yago3.1/yagoFacts.tsv.7z\n",
    "\n",
    "* A set of subclass relationships (for example, specifying that *A1086* is *a road in England*, or that *Salmonella Dub* is *a Reggae music group* and also a *New Zealand dub musical group*). Link: http://resources.mpi-inf.mpg.de/yago-naga/yago3.1/yagoTransitiveType.tsv.7z\n",
    "\n",
    "Please use `wget` to download the data to your compute engine (the files are big!).\n",
    "\n",
    "Next, you will need to extract `tsv` files from the `7z` archives that you have downloaded. Use the following commands to install `p7zip` on your compute engine and extract the files.\n",
    "```\n",
    "sudo apt-get install p7zip-full\n",
    "7z x yagoTransitiveType.tsv.7z \n",
    "7z x yagoFacts.tsv.7z \n",
    "```\n",
    "Please note that this can take a while, in particular as `yagoTransitiveType.tsv` is **18GB** large.\n",
    "\n",
    "Put the files (`yagoTransitiveType.tsv` and `yagoFacts.tsv`) into the Hadoop file system. \n",
    "Also, have a look at their first few lines to understand what kind of data they contain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_oIUo4nAFks"
   },
   "source": [
    "***\n",
    "**The command I used to download data and put data into hdfs**\n",
    "```\n",
    "wget http://resources.mpi-inf.mpg.de/yago-naga/yago3.1/yagoFacts.tsv.7z\n",
    "wget http://resources.mpi-inf.mpg.de/yago-naga/yago3.1/yagoTransitiveType.tsv.7z\n",
    "sudo apt-get install p7zip-full\n",
    "7z x yagoTransitiveType.tsv.7z \n",
    "7z x yagoFacts.tsv.7z\n",
    "hadoop fs -put ./ /yago\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPkTqjAFAFks"
   },
   "source": [
    "## 0.2 Read the data into a Spark DataFrame\n",
    "\n",
    "Please load the data from `yagoFacts.tsv` into a DataFrame called `df` and `yagoTransitiveType.tsv` into a DataFrame called `df_subclasses`.\n",
    "Have a look at the beginning of the files to understand the schema.\n",
    "Once imported, both DataFrames should have columns labelled as `id`, `subject`, `predicate`, `object` and `value`.\n",
    "In the case of `yagoTransitiveType.tsv`, some of the predicates can be understood as *\"is a sublcass of\"* or *\"is member of the class\"*, and the objects can be understood as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After having looked at the head of the file in the terminal, one can see that there is no header for the columns\n",
    "# so we'll have to create the schema ourselves. Also, the first line is an explanation of what the database contains,\n",
    "# which has to be removed before converting it into a df, so we'll upload the data to an RDD to do that removal\n",
    "# with the following function:\n",
    "def remove_first(itr_index, itr):\n",
    "    return iter(list(itr)[1:]) if itr_index == 0 else itr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data to an RDD and remove first line\n",
    "file = sc.textFile(\"/yago/yagoFacts.tsv\")\n",
    "data = file.mapPartitionsWithIndex(remove_first) # Maybe this is smarter: rdd.zipWithIndex().filter(lambda tup: tup[1] > 0).map(lambda tup: tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by columns\n",
    "rdd_map = data.map(lambda x: x.split(\"\\t\"))   \n",
    "#rdd_map.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've already seen the data and datatypes it contains, because there's no header describing the dataset\n",
    "# we create a schema\n",
    "schema = StructType([\n",
    "                StructField(\"id\",StringType()),\n",
    "                StructField(\"subject\", StringType()),\n",
    "                StructField(\"predicate\", StringType()),\n",
    "                StructField(\"object\", StringType()),\n",
    "                StructField(\"value\", StringType())#DoubleType\n",
    "                    ])\n",
    "# It didn't work with the value as a DoubleType, so we'll create it with that column as a string\n",
    "# and change the datatype afterwards\n",
    "df = sqlContext.createDataFrame(rdd_map, schema)\n",
    "#df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change value Datatype\n",
    "df = df.withColumn(\"value\", df[\"value\"].cast(DoubleType()))\n",
    "#df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the Transitive Type Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = sc.textFile(\"/yago/yagoTransitiveType.tsv\")\n",
    "data2 = file.mapPartitionsWithIndex(remove_first) # Maybe this is smarter:\n",
    "#data = file.zipWithIndex().filter(lambda tup: tup[1] > 0).map(lambda tup: tup[0])\n",
    "rdd_map2 = data2.map(lambda x: x.split(\"\\t\"))   \n",
    "schema = StructType([\n",
    "                StructField(\"id\",StringType()),\n",
    "                StructField(\"subject\", StringType()),\n",
    "                StructField(\"predicate\", StringType()),\n",
    "                StructField(\"object\", StringType()),\n",
    "                StructField(\"value\", StringType())#DoubleType\n",
    "                    ])\n",
    "# It didn't work with the value as a DoubleType, so we'll create it with that column as a string\n",
    "# and change the datatype afterwards\n",
    "df_subclasses = sqlContext.createDataFrame(rdd_map2, schema)\n",
    "df_subclasses = df_subclasses.withColumn(\"value\", df_subclasses[\"value\"].cast(DoubleType()))\n",
    "#df_subclasses.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5aQpS0hAFkt"
   },
   "source": [
    "## 0.3 Understand the database schema\n",
    "\n",
    "Let's look at the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- predicate: string (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DYvpJFWnAFku",
    "outputId": "d5eb7d89-e582-4096-d643-4a52a4e55b67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- predicate: string (nullable = true)\n",
      " |-- object: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_subclasses.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULK0aipvAFku"
   },
   "source": [
    "The useful information is in columns \"subject\", \"predicate\" and \"object\". \"predicate\" defines the relation between entities \"subject\" and \"object\". For example, for \"Albert Einstein was born in Ulm\", \"Albert Einstein\" is the subject, \"was born in\" is the predicate and \"Ulm\" is the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWlhiNH8AFkv"
   },
   "source": [
    "## 0.4 Simple query example\n",
    "\n",
    "To get information about where Albert Einstein was born, we load data into Spark using the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "SZiOTz8aAFkv",
    "outputId": "eebcd600-e82a-4de1-a356-b832a493306f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+---------------+-----+\n",
      "|                  id|             subject|  predicate|         object|value|\n",
      "+--------------------+--------------------+-----------+---------------+-----+\n",
      "|<id_thPX9b1zg!_7f...|<William_Jones_(W...|<wasBornIn>|<Penrhiwceiber>| null|\n",
      "+--------------------+--------------------+-----------+---------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "born_city_df = df.where(\"predicate == '<wasBornIn>'\")\n",
    "born_city_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gPKj9hu3AFkw",
    "outputId": "b5cf2fea-3c1e-456b-abb5-b48f5b48ef72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+-----------+------+-----+-----+\n",
      "|                  id|          subject|  predicate|object|value|label|\n",
      "+--------------------+-----------------+-----------+------+-----+-----+\n",
      "|<id_sbCVliqDT2_7f...|<Albert_Einstein>|<wasBornIn>| <Ulm>|     | null|\n",
      "+--------------------+-----------------+-----------+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "born_city_df.where(\"subject = '<Albert_Einstein>'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k88MMFJ-AFkw"
   },
   "source": [
    "You may wonder how one would know whether to use the predicate '&lt;wasBornIn&gt;' or '&lt;was_born_in&gt;' and subject '&lt;Albert_Einstein&gt;' or '&lt;AlbertEinstein&gt;'. For YAGO subjects (and objects), the naming is aligned with Wikipedia. For example, Albert Einstein's wiki is: https://en.wikipedia.org/wiki/Albert_Einstein and you can see it is 'Albert_Einstein'. \n",
    "\n",
    "For predicates, you can look at the \"property\" list from the [Yago Web interface](https://gate.d5.mpi-inf.mpg.de/webyagospotlx/WebInterface?L01=%3Fx&L0R=%3CwasBornIn%3E&L02=%3Fc&L0T=&L03=&L0L=&L04=&L05=&L11=&L1R=&L12=&L1T=&L13=&L1L=&L14=&L15=&L21=&L2R=&L22=&L2T=&L23=&L2L=&L24=&L25=&L31=&L3R=&L32=&L3T=&L33=&L3L=&L34=&L35=&L41=&L4R=&L42=&L4T=&L43=&L4L=&L44=&L45=). \n",
    "Try different queries with this web interface query to understand more how to query YAGO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uLuSmGuAFkw"
   },
   "source": [
    "## 0.5 Simple motif example\n",
    "\n",
    "In this part of the assignment, you are required to use **motif** to find out answer to the 4 questions. Please complete the following example to find out: \"Which city was Albert Einstein born in?\" using motif queries instead of  SQL queries on the first dataframe (`df`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import graphframes\n",
    "from graphframes import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------+\n",
      "|                  a|                  ab|      b|\n",
      "+-------------------+--------------------+-------+\n",
      "|[<Albert_Einstein>]|[<Albert_Einstein...|[<Ulm>]|\n",
      "+-------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def vertices_edges_split(df, condition):\n",
    "    sub = df.filter(condition).select(\"subject\").withColumnRenamed(\"subject\",\"id\")\n",
    "    obj = df.filter(condition).select(\"object\").withColumnRenamed(\"object\",\"id\")\n",
    "    v = sub.union(obj).distinct()\n",
    "    e = df.filter(condition).select(\"subject\",\"object\",\"predicate\")\\\n",
    "    .withColumnRenamed(\"subject\",\"src\").withColumnRenamed(\"object\",\"dst\")\n",
    "    return v, e\n",
    "\n",
    "v, e = vertices_edges_split(df, \"subject='<Albert_Einstein>'\")\n",
    "g = GraphFrame(v, e)\n",
    "# your code\n",
    "\n",
    "res = g.find(\"(a)-[ab]->(b)\").filter(\"b.id = '<Ulm>'\")\n",
    "res.show() # took 2min 30sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other operations with graphs for the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphFrame(v:[id: string], e:[src: string, dst: string ... 1 more field])\n",
      "+-----+\n",
      "|   id|\n",
      "+-----+\n",
      "|<Ulm>|\n",
      "+-----+\n",
      "\n",
      "+-------------+-----+\n",
      "|    predicate|count|\n",
      "+-------------+-----+\n",
      "| <influences>|   25|\n",
      "|    <worksAt>|   12|\n",
      "|<isCitizenOf>|   12|\n",
      "|<hasWonPrize>|    8|\n",
      "|<isMarriedTo>|    2|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+--------------------+-----------------+\n",
      "|                  a|                  ab|                b|\n",
      "+-------------------+--------------------+-----------------+\n",
      "|[<Albert_Einstein>]|[<Albert_Einstein...|[<Elsa_Einstein>]|\n",
      "|[<Albert_Einstein>]|[<Albert_Einstein...| [<Mileva_Marić>]|\n",
      "+-------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(g)\n",
    "g.vertices.count()\n",
    "g.edges.count()\n",
    "g.find(\"(a)-[e]->(b)\").count()\n",
    "g.vertices.filter(\"id = '<Ulm>'\").show(1)\n",
    "g.edges.groupBy(\"predicate\").count().sort(\"count\", ascending = False).show(5) # Up until here 6min 15sec\n",
    "\n",
    "# Another Motiv query\n",
    "g.find(\"(a)-[ab]->(b)\").filter(\"ab.predicate = '<isMarriedTo>'\").show() # 8min 20sec... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmd1E8UGAFkx"
   },
   "source": [
    "## 0.6 Some useful tips\n",
    "\n",
    "### Get a subset of YAGO database\n",
    "YAGO database is large, so we don't try to load the entire database into a dataframe and then query it. If you do this, you will find that you won't even be able to execute `df.take(1)`, as it would take up too much of space (at least on a laptop). Instead, you use Spark SQL commands or `df.where` to get a suitable fraction of the data.\n",
    "\n",
    "### Try the queries in the YAGO Web interface first\n",
    "It is sometimes tricky to get the right \"subject\", \"predicate\" and \"object\". It is easier if you start from [Yago Web interface](https://gate.d5.mpi-inf.mpg.de/webyagospotlx/WebInterface?L01=%3Fx&L0R=%3CwasBornIn%3E&L02=%3Fc&L0T=&L03=&L0L=&L04=&L05=&L11=&L1R=&L12=&L1T=&L13=&L1L=&L14=&L15=&L21=&L2R=&L22=&L2T=&L23=&L2L=&L24=&L25=&L31=&L3R=&L32=&L3T=&L33=&L3L=&L34=&L35=&L41=&L4R=&L42=&L4T=&L43=&L4L=&L44=&L45=) rather than directly querying in Pyspark. Once your query works, you can convert your query to Pyspark code. Note that sometimes the web version of object/subject code may be different from what you need to type here. For example, company code is &lt;wordnet_company_108058098&gt; when you do the query here but when you do it via the web interface it is &lt;wordnet company 108058098&gt;. \n",
    "\n",
    "### Be patient and don't do this exercise in the last minute\n",
    "Some trial and error is needed to get the query right and it may take some time get the result for a query. For these reasons, we advise you not to wait to work out this exercise just before the submission deadline. \n",
    "\n",
    "### Make sure to get the initialization actions right\n",
    "For this exercise, you will be using GraphFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcG2wA0eAFky"
   },
   "source": [
    "## 1. Politicians who are also scientists (Question A)\n",
    "Find all politicians who are also scientists. Output top 20 of them. How many people are in the dataset who are both scientists and politicians?\n",
    "Please follow these steps:\n",
    "* Operate on the subsets of `df_subclasses` where the objects are `'<wordnet_scientist_110560637>` (scientists) and `'<wordnet_politician_110450303>'` (politicians), and where the predicates are `rdf:type`.\n",
    "* Use graphframes and the right parts of `df_subclasses` to construct a graph whose (directed) edges point from subjects to objects. Hence, its source vertices are subjects and it destination vertices are objects. It may be convenient to use intermediate DataFrames and join all the required dataframes of edges and vertices.\n",
    "* The subjects will be people and the objects will be classes (e.g., scientists, politicians).\n",
    "* Use a motif query to find all instances that fulfil the criteria specified in the question.\n",
    "* It is a good idea to define a function that takes a DataFrame and outputs a set of data frames for vertices and edges.\n",
    "\n",
    "Please sort the output alphabetically by the person column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sci_pol = df_subclasses.where(\"object = '<wordnet_scientist_110560637>' or object = '<wordnet_politician_110450303>'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scientists_df = df_subclasses.filter(\"object = '<wordnet_scientist_110560637>'\")\\\n",
    ".filter(\"predicate = 'rdf:type'\").select(\"subject\",\"object\",\"predicate\").withColumnRenamed(\"predicate\", \"pred_sci\")\\\n",
    ".withColumnRenamed(\"object\", \"obj_sci\")\n",
    "\n",
    "\n",
    "politicians_df = df_subclasses.filter(\"object = '<wordnet_politician_110450303>'\")\\\n",
    ".filter(\"predicate = 'rdf:type'\").select(\"subject\",\"object\",\"predicate\").withColumnRenamed(\"predicate\", \"pred_pol\")\\\n",
    ".withColumnRenamed(\"object\", \"obj_pol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "polisci_df = scientists_df.join(politicians_df, on=['subject'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = polisci_df.select(\"subject\").distinct().withColumnRenamed(\"subject\",\"id\")\n",
    "dst_sci = polisci_df.select(\"obj_sci\").distinct().withColumnRenamed(\"obj_sci\",\"id\")\n",
    "dst_pol = polisci_df.select(\"obj_pol\").distinct().withColumnRenamed(\"obj_pol\",\"id\")\n",
    "\n",
    "v = src.unionAll(dst_sci).unionAll(dst_pol)\n",
    "\n",
    "sci_edg = polisci_df.select(\"subject\", \"obj_sci\", \"pred_sci\").withColumnRenamed(\"subject\",\"src\")\\\n",
    ".withColumnRenamed(\"pred_sci\",\"pred\").withColumnRenamed(\"obj_sci\",\"dst\")\n",
    "pol_edg = polisci_df.select(\"subject\", \"obj_pol\", \"pred_pol\").withColumnRenamed(\"subject\",\"src\")\\\n",
    ".withColumnRenamed(\"pred_pol\",\"pred\").withColumnRenamed(\"obj_pol\",\"dst\")\n",
    "\n",
    "e = sci_edg.unionAll(pol_edg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polsciGraph = GraphFrame(v, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                   a|                  ab|                   b|                  ac|                   c|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      [<A._C._Cuza>]|[<A._C._Cuza>, <w...|[<wordnet_scienti...|[<A._C._Cuza>, <w...|[<wordnet_politic...|\n",
      "|[<A._P._J._Abdul_...|[<A._P._J._Abdul_...|[<wordnet_scienti...|[<A._P._J._Abdul_...|[<wordnet_politic...|\n",
      "|       [<Aad_Kosto>]|[<Aad_Kosto>, <wo...|[<wordnet_scienti...|[<Aad_Kosto>, <wo...|[<wordnet_politic...|\n",
      "|        [<Aad_Nuis>]|[<Aad_Nuis>, <wor...|[<wordnet_scienti...|[<Aad_Nuis>, <wor...|[<wordnet_politic...|\n",
      "| [<Aaron_Aaronsohn>]|[<Aaron_Aaronsohn...|[<wordnet_scienti...|[<Aaron_Aaronsohn...|[<wordnet_politic...|\n",
      "|  [<Aaron_Farrugia>]|[<Aaron_Farrugia>...|[<wordnet_scienti...|[<Aaron_Farrugia>...|[<wordnet_politic...|\n",
      "|        [<Ab_Klink>]|[<Ab_Klink>, <wor...|[<wordnet_scienti...|[<Ab_Klink>, <wor...|[<wordnet_politic...|\n",
      "|  [<Abba_P._Lerner>]|[<Abba_P._Lerner>...|[<wordnet_scienti...|[<Abba_P._Lerner>...|[<wordnet_politic...|\n",
      "|[<Abbas_Ahmad_Akh...|[<Abbas_Ahmad_Akh...|[<wordnet_scienti...|[<Abbas_Ahmad_Akh...|[<wordnet_politic...|\n",
      "|   [<Abbie_Hoffman>]|[<Abbie_Hoffman>,...|[<wordnet_scienti...|[<Abbie_Hoffman>,...|[<wordnet_politic...|\n",
      "|[<Abbott_Lawrence...|[<Abbott_Lawrence...|[<wordnet_scienti...|[<Abbott_Lawrence...|[<wordnet_politic...|\n",
      "|[<Abdallah_Salem_...|[<Abdallah_Salem_...|[<wordnet_scienti...|[<Abdallah_Salem_...|[<wordnet_politic...|\n",
      "|[<Abdelbaki_Herma...|[<Abdelbaki_Herma...|[<wordnet_scienti...|[<Abdelbaki_Herma...|[<wordnet_politic...|\n",
      "| [<Abdellatif_Abid>]|[<Abdellatif_Abid...|[<wordnet_scienti...|[<Abdellatif_Abid...|[<wordnet_politic...|\n",
      "|[<Abdelouahed_Sou...|[<Abdelouahed_Sou...|[<wordnet_scienti...|[<Abdelouahed_Sou...|[<wordnet_politic...|\n",
      "| [<Abdelwahed_Radi>]|[<Abdelwahed_Radi...|[<wordnet_scienti...|[<Abdelwahed_Radi...|[<wordnet_politic...|\n",
      "|[<Abdesslam_Yassi...|[<Abdesslam_Yassi...|[<wordnet_scienti...|[<Abdesslam_Yassi...|[<wordnet_politic...|\n",
      "|[<Abdi_Farah_Shir...|[<Abdi_Farah_Shir...|[<wordnet_scienti...|[<Abdi_Farah_Shir...|[<wordnet_politic...|\n",
      "|[<Abdirahman_Dual...|[<Abdirahman_Dual...|[<wordnet_scienti...|[<Abdirahman_Dual...|[<wordnet_politic...|\n",
      "|[<Abdiweli_Mohame...|[<Abdiweli_Mohame...|[<wordnet_scienti...|[<Abdiweli_Mohame...|[<wordnet_politic...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "polsciGraph.find(\"(a)-[ab]->(b); (a)-[ac]->(c)\").filter(\"b.id = '<wordnet_scientist_110560637>'\")\\\n",
    ".filter(\"b.id != c.id\").sort(\"a\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wEOj9sJAFk0"
   },
   "source": [
    "## 2. Companies whose founders were born in London (Question B)\n",
    "For companies, use `'<wordnet_company_108058098>'`. \n",
    "For *\"being founder\"*, use `<created>`.\n",
    "\n",
    "By now, you will understand which DataFrame to use for what. \n",
    "\n",
    "Set up a graph and use a motif query to find companies whose founders were born in London.\n",
    "Please take some time to figure out how a suitable configuration of nodes and edges should look like.  How many such companies are there in our dataset?\n",
    "\n",
    "Please sort the output alphabetically by the founder column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subclasses.createOrReplaceTempView(\"df_subclasses\")\n",
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_found = spark.sql(\"SELECT s.subject as company, s.object as comp_obj, s.predicate as comp_pred, \\\n",
    "d.subject as founder, d.predicate as found_pred \\\n",
    "FROM df_subclasses s JOIN df d ON s.subject = d.object \\\n",
    "WHERE s.predicate = 'rdf:type' AND s.object = '<wordnet_company_108058098>' AND d.predicate = '<created>'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_found.createOrReplaceTempView(\"comps_found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_found_Ldn = spark.sql(\"SELECT cf.company, cf.comp_pred, cf.comp_obj, cf.founder, cf.found_pred, \\\n",
    "l.object as london, l.predicate as ldn_pred \\\n",
    "FROM comps_found cf JOIN df l ON cf.founder = l.subject \\\n",
    "WHERE l.predicate = '<wasBornIn>' AND l.object = '<London>'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps_found_Ldn.createOrReplaceTempView(\"comps_found_Ldn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = comps_found_Ldn.select(\"company\",).withColumnRenamed(\"company\", \"id\")\\\n",
    ".union(comps_found_Ldn.select(\"comp_obj\",).withColumnRenamed(\"comp_obj\", \"id\"))\\\n",
    ".union(comps_found_Ldn.select(\"founder\",).withColumnRenamed(\"founder\", \"id\"))\\\n",
    ".union(comps_found_Ldn.select(\"london\",).withColumnRenamed(\"london\", \"id\")).distinct()\n",
    "\n",
    "\n",
    "e = comps_found_Ldn.select(\"company\", \"comp_obj\", \"comp_pred\").withColumnRenamed(\"company\", \"src\")\\\n",
    ".withColumnRenamed(\"comp_obj\", \"dst\").withColumnRenamed(\"comp_pred\", \"pred\")\\\n",
    ".union(comps_found_Ldn.select(\"founder\", \"company\", \"found_pred\").withColumnRenamed(\"founder\", \"src\")\\\n",
    ".withColumnRenamed(\"company\", \"dst\").withColumnRenamed(\"found_pred\", \"pred\"))\\\n",
    ".union(comps_found_Ldn.select(\"founder\", \"london\", \"ldn_pred\").withColumnRenamed(\"founder\", \"src\")\\\n",
    ".withColumnRenamed(\"london\", \"dst\").withColumnRenamed(\"ldn_pred\", \"pred\")).distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ldn = GraphFrame(v, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginning = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|                   c|                  e1|                   t|                   f|                  e2|                  e3|         l|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     [<Dare_Comics>]|[<Dare_Comics>, <...|[<wordnet_company...|      [<Adam_Hamdy>]|[<Adam_Hamdy>, <D...|[<Adam_Hamdy>, <L...|[<London>]|\n",
      "|[<Jawbone_(compan...|[<Jawbone_(compan...|[<wordnet_company...|[<Alexander_Assei...|[<Alexander_Assei...|[<Alexander_Assei...|[<London>]|\n",
      "|      [<Video_Arts>]|[<Video_Arts>, <w...|[<wordnet_company...|      [<Antony_Jay>]|[<Antony_Jay>, <V...|[<Antony_Jay>, <L...|[<London>]|\n",
      "|[<SENS_Research_F...|[<SENS_Research_F...|[<wordnet_company...|  [<Aubrey_de_Grey>]|[<Aubrey_de_Grey>...|[<Aubrey_de_Grey>...|[<London>]|\n",
      "|[<Andreessen_Horo...|[<Andreessen_Horo...|[<wordnet_company...|    [<Ben_Horowitz>]|[<Ben_Horowitz>, ...|[<Ben_Horowitz>, ...|[<London>]|\n",
      "|  [<LO-MAX_Records>]|[<LO-MAX_Records>...|[<wordnet_company...|[<Bernard_MacMaho...|[<Bernard_MacMaho...|[<Bernard_MacMaho...|[<London>]|\n",
      "|        [<PowerBar>]|[<PowerBar>, <wor...|[<wordnet_company...|   [<Brian_Maxwell>]|[<Brian_Maxwell>,...|[<Brian_Maxwell>,...|[<London>]|\n",
      "|[<Primrose_Hill_P...|[<Primrose_Hill_P...|[<wordnet_company...|    [<Bruno_Heller>]|[<Bruno_Heller>, ...|[<Bruno_Heller>, ...|[<London>]|\n",
      "|  [<United_Artists>]|[<United_Artists>...|[<wordnet_company...| [<Charlie_Chaplin>]|[<Charlie_Chaplin...|[<Charlie_Chaplin...|[<London>]|\n",
      "|[<Kurrupt_Recordi...|[<Kurrupt_Recordi...|[<wordnet_company...|       [<Dan_Joyce>]|[<Dan_Joyce>, <Ku...|[<Dan_Joyce>, <Lo...|[<London>]|\n",
      "|[<Three_Rings_Des...|[<Three_Rings_Des...|[<wordnet_company...|[<Daniel_James_(g...|[<Daniel_James_(g...|[<Daniel_James_(g...|[<London>]|\n",
      "|         [<I-Logix>]|[<I-Logix>, <word...|[<wordnet_company...|     [<David_Harel>]|[<David_Harel>, <...|[<David_Harel>, <...|[<London>]|\n",
      "|    [<Heyday_Films>]|[<Heyday_Films>, ...|[<wordnet_company...|    [<David_Heyman>]|[<David_Heyman>, ...|[<David_Heyman>, ...|[<London>]|\n",
      "|[<Israel_Council_...|[<Israel_Council_...|[<wordnet_company...|    [<David_Kimche>]|[<David_Kimche>, ...|[<David_Kimche>, ...|[<London>]|\n",
      "|        [<DeepMind>]|[<DeepMind>, <wor...|[<wordnet_company...|  [<Demis_Hassabis>]|[<Demis_Hassabis>...|[<Demis_Hassabis>...|[<London>]|\n",
      "|  [<Scripps_Health>]|[<Scripps_Health>...|[<wordnet_company...|[<Ellen_Browning_...|[<Ellen_Browning_...|[<Ellen_Browning_...|[<London>]|\n",
      "|[<Luce,_Forward,_...|[<Luce,_Forward,_...|[<wordnet_company...|[<Ellen_Browning_...|[<Ellen_Browning_...|[<Ellen_Browning_...|[<London>]|\n",
      "| [<Blossöm_Records>]|[<Blossöm_Records...|[<wordnet_company...|    [<Elliot_James>]|[<Elliot_James>, ...|[<Elliot_James>, ...|[<London>]|\n",
      "|    [<Syncopy_Inc.>]|[<Syncopy_Inc.>, ...|[<wordnet_company...|     [<Emma_Thomas>]|[<Emma_Thomas>, <...|[<Emma_Thomas>, <...|[<London>]|\n",
      "|[<Rezolution_Pict...|[<Rezolution_Pict...|[<wordnet_company...|     [<Ernest_Webb>]|[<Ernest_Webb>, <...|[<Ernest_Webb>, <...|[<London>]|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g_ldn.find(\"(c)-[e1]->(t); (f)-[e2]->(c); (f)-[e3]->(l)\").filter(\"t.id = '<wordnet_company_108058098>'\")\\\n",
    ".filter(\"e2.pred = '<created>'\").filter(\"l.id = '<London>'\").sort(\"f\").show(20) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 10:52:23 \n",
      " Ended at 13:14:31\n",
      "('Took', '-1 day, 21:37:52.715393')\n"
     ]
    }
   ],
   "source": [
    "ending = datetime.now()\n",
    "beginning_time = beginning.strftime(\"%H:%M:%S\")\n",
    "ending_time = ending.strftime(\"%H:%M:%S\")\n",
    "\n",
    "print(\"Started at {} \\n Ended at {}\".format(beginning_time, ending_time))\n",
    "elapsed = (beginning-ending)\n",
    "print(\"Took\", str(elapsed))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "hw_yago_local.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
